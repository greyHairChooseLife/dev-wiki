#  DEV WIKI


-  내 두번째 번호 010 3039 7704


> _이 위키의 목적은 기록하여 필요시 되찾아보기 위함이다.
> 누군가에게 뽐내기 위한 용도가 아니다._


> _"더 열심히 해"_ from.고은상


> _프로덕션에 가까운 프로젝트들을 보면서 배우는 것이 중요하다._
> _"Fediverse가 프로덕션에 가깝게 되어있는게 많아요. 루비는 마스토돈, nodejs는 미스키, 파이썬은 bookwyrm"_


> _Make note? Do PARA!_
>
> PARA 메모법:_실행도를 기준으로 노트를 분류하는 기법. 주제별로 나누는 것이 아니다._
>
>   - Project:
>     - **지금 가장 전념**하는 것
>     - 기간/목표가 명확함. 즉, 끝이 있음
>     - ex) 이사하기, 직장 취업하기, 책 쓰기
>
>   - Area:
>     - **끝이 없고, 꾸준히** 해 나가는 것
>     - 나의 책임이나 역할이 있음
>     - ex) 운동, 독서, 프로그래밍 실력 향상
>
>   - Resource:
>     - 관심 분야 정보
>     - 내 **책임은 없음**
>     - ex) 창업, 종교, 예술, 춤추는법 등
>
>   - Archive:
>     - 위의 세 가지에 속하지 않는 것
>     - 종료/중단된 프로젝트
>     - 책임이 사라진 영역, 관심이 사라진 자원

[-]
https://github.com/jmacadie/telescope-hierarchy.nvim
https://www.reddit.com/r/neovim/comments/1h25lal/what_are_your_favorite_underappreciated_neovim/
https://github.com/rachartier/tiny-inline-diagnostic.nvim
https://github.com/Wansmer/treesj
https://www.google.com/search?q=codecompanion%20vs%20avante%20vs%20copilotchat
https://github.com/AndreM222/copilot-lualine
https://github.com/mikavilpas/yazi.nvim

& devdocs.nvim
& don't use copilot anymore, just supermaven



[-]
https://www.youtube.com/watch?v=OElI_q4qDF0
https://www.youtube.com/watch?v=k7ouok3qks0
https://www.youtube.com/watch?v=XcLkGT5pe2w
https://www.youtube.com/watch?v=20GQNkZpe-U
https://www.reddit.com/r/neovim/comments/1jrpbi9/talk_with_lazar_nikolov_software_engineer/

## Project


[CBPM 견적서 관리 웹앱 개발](/Project/CBPM_견적서_관리_웹앱_개발/index.md)

[결혼에 확신을 갖는 방법 찾기](/Project/결혼에_확신을_갖는_방법_찾기/index.md)

[성금 엄마아빠집 리모델링](/Project/성금_엄마아빠집_리모델링/index.md)

[만다라트](/Project/만다라트/index.md)
  - 2차전에 돌입했다.

[anthropic API & avante.nvim problem](/Project/anthropic_API_&_avante.nvim_problem)


### Pending

- 차량 관리
  - 코란도 엔진오일
  - 경비 아저씨가 나 대신 ..
  - 관련해서 스트레스를 준 아저씨가 있다. 골프장 아저씨


- neovim `tohtml.vim` 개선

  > `render-markdown.nvim`으로 렌더링(하이라이팅) 된 것을 있는 그대로 pdf 또는 이미지로 출력하고싶었다.
  > `TOhtml` 명령을 이용해보란 조언을 얻었다.(vim.kr 디스코드 채널)
  >
  > 그런데 vimwiki에서 당장 TOhtml 해보면 에러가 발생하는거다. 이것에 대해 대화하다가 아래와같은 조언을
  > 얻었다.
  >
  > ```txt
  > tohtml이 루아로 변환된지 오래되지 않아서 어쩌면 네오빔 업스트림에 기여하실수도있어요
  > 버그 고치신다면 리뷰하고 받아주실거같아요
  > html 나온거에서 </pre> 에 해당하는 부분
  > 그 부분 계산하다가 태그가 안맞아서 뻗은거네요
  > <pre> .. </pre> 사이에 태그가 뭔가 빠졌다는건데
  > ```
  >
  > [tohtml.vim](https://github.com/neovim/neovim/blob/3d3a99e69cda365cae9ad65831712301807a772b/runtime/lua/tohtml.lua)


## Area [-]

### Programming [-]

#### 살아남는 개발자는 무엇이 다른가? [-]

> - 쉽게 변하지 않는 것들에 집중하자
> - 시키는 대로 하기 전에 의심해라. 정말로 필요한 과정/단계/방법인가? 똑똑하고 신뢰할만한 사람일수록
> - recipe를 존중하되, 본질을 반드시 이해해라

[개발자 중 누가 살아 남을까? 생존을 위해 반드시 생각해야 할 점](/Area/Programming/살아남는_개발자는_무엇이_다른가?/개발자_중_누가_살아_남을까?_생존을_위해_반드시_생각해야_할_점)
[개발자가 앞으로 꼭 공부해야하는 분야](/Area/Programming/살아남는_개발자는_무엇이_다른가?/개발자가_앞으로_꼭_공부해야하는_분야)
[일론의 5단계 법칙](/Area/Programming/살아남는_개발자는_무엇이_다른가?/일론의_5단계_법칙)
[일류와 이류의 차이 (반도체 전설 짐켈러)](/Area/Programming/살아남는_개발자는_무엇이_다른가?/일류와_이류의_차이_(반도체_전설_짐켈러))


[-]
[The LinkedIn DPH Framework](https://linkedin.github.io/dph-framework/)

[-]
[지식근로자의 생산성](https://www.joinc.co.kr/w/knowledge-workder-productivity)


#### 구독 서비스 관리

| Category     | Service        | Cost      |
|--------------|----------------|-----------|
| Subscription | GitHub Copilot | $100/Y    |
|              | OpenAI GPT     | $20/M     |
|              | Figma          | $15/M     |
| API          | Anthropic      | claude.ai |


#### Note Making [-]

[Job-wiki 작성법](/Area/Programming/Note_Making/Job-wiki_작성법)

> [!INFO]
>
> [vim-wiki cheet-sheet](https://gist.github.com/drkarl/4c503bccb62558dc85e8b1bc0f29e9cb)
> [design reference](https://mkaz.blog/)
> - 현재 디렉토리 내의 파일명에 공백이 있을 때, 공백을 '_'(under-bar)로 변경하기
>   ```bash
>   for file in *\ *; do mv "$file" "${file// /_}"; done
>   ```


#### Remote Work

[file share](/Area/Programming/Remote_Work/file_share)
[NFS protocol, 디렉토리 공유하기](/Area/Programming/Remote_Work/NFS_protocol,_디렉토리_공유하기)
[SSH protocol](/Area/Programming/Remote_Work/SSH_protocol)
[rclone으로 클라우드 스토리지 서비스 동기화](/Area/Programming/Remote_Work/rclone으로_클라우드_스토리지_서비스_동기화)

#### Operating System [-]

[HUB: Operating System](/Area/Programming//Operating_System/index.md)

1. 설치&복제
2. 서비스 관리
3. system Packages & Apps
4. Shell & Terminal & Editor
5. secretes 관리
6. etc

#### Skills [-]

[HUB: Skills](/Area/Programming/Skills/index.md)

1. Container
2. Web Dev
3. Chrome Extension
4. Cloud Computing
5. CI CD
6. Languages
7. Web Scraping
8. Big Data
9. Database

#### Experience [-]

##### Architecture [-]

[-]
내가 그동안 설계한 것들 정리해두면 좋겠다.


##### Debugging

- 미결
  [docker named volume with NFS protocol](/Area/Programming/Experience/Debugging/docker_named_volume_with_NFS_protocol)


- 해결
  [domain status: clientHold](/Area/Programming/Experience/Debugging/domain_status:_clientHold)
  [프로세스 조회 및 종료](/Area/Programming/Experience/Debugging/프로세스_조회_및_종료)

##### etc

[vim을 왜 쓸까?](/Area/Programming/Experience/etc/vim을_왜_쓸까?)
[gitsigns.nvim plugin에 이슈 남기기](/Area/Programming/Experience/etc/gitsigns.nvim_plugin에_이슈_남기기)
[nvim 플러그인에 이슈 남기기](/Area/Programming/Experience/etc/nvim_플러그인에_이슈_남기기)
[오픈소스에 난생처음 PR 날리기](/Area/Programming/Experience/etc/오픈소스에_난생처음_PR_날리기)


#### AI [-]


[-]
[“AI는 없는 인간만의 뭐가 있다? 그런 말 하는 사람 다 밀려났다.” (장강명 작가)](https://www.youtube.com/watch?v=bBaTc1JE42w)


- 사용중
  [aider](/Area/Programming/AI/aider)
  - avante.nvim
  - github copilot with ChatCopilot.nvim
  - chatGPT web UI
  - > [!lg] Log 2025-03-19
    >
    > avante.nvim vs ChatCopilot.nvim?
    > : AI api provider에 직접 접근하냐, github에서 랩핑해서 배포하는걸 사용하냐의 차이
    > 비용적으로는 copilot이 우위가 있으나 성능은 또 모르겠다. 일단은 지켜보는 수밖에?


- 관심
  - devin(commercial) & [devica(open-source)](https://github.com/stitionai/devika/discussions)
    : agent-like
  - codecompanion.nvim
    : zed ai를 표방한다는데, 'cursor AI' 스타일보다 vim스럽긴 하다고 생각한다. 그러나 ai의
    활용 면에서 나는 cursor AI 스타일을 선호한다.


- ETC
  [what is chatGPT?](/Area/Programming/AI/what_is_chatGPT?)


#### 알고리즘

[알고리즘 게임: CodinGame](https://www.codingame.com/training)

- 가이드라인 (discord에서 주워들었다.)

  ```txt
  1. 근데 알고리즘도 뭐 그렇게 어렵지는 않고, 아래에서 크게 벗어나지는 않더라구요.
     - 문법 숙지 -> 브루트포스 -> 백트래킹 -> 자료구조 공부(STL) -> DFS -> BFS -> 다익스트라 ->
       유니온파인드 -> MST (이후)
     - 정렬
     - 이분탐색
     - 그리디
     - 기본적인 DP
     - 비트마스킹
     - 유클리드호제법
     - 에라토스테네스의 체

  2. 저희 회사도 막 신박한 희안한 알고리즘 가져와서 풀어야되는걸 내기보다는 알고리즘을 제대로
     이해했는지, 제약사항이 주어졌을 때 어떻게 개선할 수 있고 얼마나 개선됐는지 설명할 수 있는지 이런걸
     봐요. Big O 노테이션으로다가.

  3. 리트코드가 신빙성 있고, medium 정도면 차고도 넘침
  ```

#### REFERENCE

- ETC
  [7 Habits For Effective Text Editing](https://www.youtube.com/watch?v=eX9m3g5J-XA)
  [양질의 블로그](https://www.joinc.co.kr/w/architecture)
  [`TS`, `python` 등 다양한 주제 튜토리얼](https://www.squash.io/tutorials/)
  [The twelve-factor app](https://12factor.net/)
  [Harvard CS 50 (2023) – Full Computer Science University Course](https://www.youtube.com/watch?v=LfaMVlDaQ24)

- backend course
  [web-course](https://www.boot.dev/tracks/backend)
  [github](https://github.com/bootdotdev/curriculum)



### 좋은 가족, 좋은 친구, 멋진 인간 하기

[선물 하고싶은 것](/Area/좋은_가족,_좋은_친구,_멋진_인간_하기/선물_하고싶은_것)
[감사한 마음](/Area/좋은_가족,_좋은_친구,_멋진_인간_하기/감사한_마음)
[다른 입장을 이해해 보기](/Area/좋은_가족,_좋은_친구,_멋진_인간_하기/다른_입장을_이해해_보기)



### 글쓰기 [-]

> [!rf]
>
> [글쓰기 레퍼런스 모음](/Area/글쓰기/뭐든지/글쓰기_레퍼런스_모음)


#### 뭐든지

[대학교를 중퇴한 이유가 뭔가요?](/Area/글쓰기/뭐든지/대학교를_중퇴한_이유가_뭔가요?)
[10시 이후로는 언어정보의 input을 제한한다.](/Area/글쓰기/뭐든지/10시_이후로는_언어정보의_input을_제한한다.)
[우리는 상대방을 눈앞에 두고도 그 실체를 보지 못한다. 단지, 내 방식대로 이해한 허상을 바라볼 뿐이다.](/Area/글쓰기/뭐든지/우리는_상대방을_눈앞에_두고도_그_실체를_보지_못한다._단지,_내_방식대로_이해한_허상을_바라볼_뿐이다.)
[살아오면서 겪어본 어려운 문제에 대하여](/Area/글쓰기/뭐든지/살아오면서_겪어본_어려운_문제에_대하여)


#### 유투브 시청 [-]

['아들 학폭' 복수 나선 아빠, 판사도 "그럴 만했네.." 무죄 (2024.12.21/MBC뉴스)](/Area/글쓰기/유투브_시청/'아들_학폭'_복수_나선_아빠,_판사도_"그럴_만했네.."_무죄)
[연금개혁으로 꿀빨았다는 50대, 과연 안 억울할까? (변명)](/Area/글쓰기/유투브_시청/연금개혁으로_꿀빨았다는_50대,_과연_안_억울할까?_(변명))

[-]
[사랑하는 방법(팔란티어 접근 방식)](https://www.youtube.com/watch?v=zyXgqQkWULU)
- 이 사람은 구독하고 잘 봐야겠다.
- 심지어 투자를 하고싶은 마음까지 들게 만든다.

[-]
[조건을 안보기 시작한 IT 업계](https://www.youtube.com/watch?v=M8oj2ZIoYbY)


### 나를 사랑하기

#### 운동

- 부분 운동
  1, [팔 이두, 10m](https://www.youtube.com/watch?v=BfZw0qMoa1A)
  1, [팔 삼두, 10m](https://www.youtube.com/watch?v=DqnWLrrO0xY)
  [팔 전완, 10m](https://www.youtube.com/watch?v=EpCKQsydB2s)
  [가슴, 10m](https://www.youtube.com/watch?v=E7OAhVIbkbk)
  1, [등, 10m](https://www.youtube.com/watch?v=BGd4sKOcl5I)
  [어깨, 10m](https://www.youtube.com/watch?v=3smk7LGLTXE)
  [배, 8m](https://www.youtube.com/watch?v=J90dERWZNUE)

- 전신 운동
  [전신, 12m](https://www.youtube.com/watch?v=vl4S_e_dBuk)
  [전신, 10m](https://www.youtube.com/watch?v=8wEdqDDzHpc)
  [전신, 8m](https://www.youtube.com/watch?v=C80QyNlAMss)
  [전신, 7m](https://www.youtube.com/watch?v=g-nzEPqqrx8)
  1, [전신, 6m](https://www.youtube.com/watch?v=LDb5Y4Ti6Bc)
  1, [전신, 5m](https://www.youtube.com/watch?v=XeWwQK-I2Tg)


### 민주 시민의 책임 & 권한 & 상식 [-]

[세금 납부](/Area/민주_시민의_책임_&_권한_&_상식/세금_납부) [-]
[각종 서류](/Area/민주_시민의_책임_&_권한_&_상식/각종_서류)


## Resource

### 프로젝트 아이디어


- keepbuffer.nvim :

  나는 gq 또는 qq로 버퍼를 종료시킨다. qq로 종료는 임시로 종료하고, 버퍼에는 그대로 남겨둔다는
  의미이다. 근데 이게 되살리려고 버퍼 리스트를 조회하면 다른 텝에서 관리하던 버퍼들까지 모두
  나타난다.

  이에 아래와같은 플러그인을 간단히 개발해 사용하면 좋겠다.

  - qq로 종료시 nvim-tree 아래 윈도우 및 버퍼 생성하며, 파일명만 표시
  - 해당 탭에서 qq로 종료 시에만 나타나야하고, 다른 텝과는 아무 상관 없음
  - 생성된 윈도우에서 파일명에 커서를 두고 열면 최근 윈도우의 우측에 다시 윈도우 생성


- 스타크래프트 ai 어시스턴트
  게임 프로세스 및 메모리를 분석해서 의사결정 등에 음성으로 도움을 준다.





- 단순 아이디어
  [독서모임](/Resource/프로젝트_아이디어/독서모임)
  [조직문화를 판매한다. 그것을 뒷받침 할 수 있는 system과 합쳐서.](/Resource/프로젝트_아이디어/조직문화를_판매한다._그것을_뒷받침_할_수_있는_system과_합쳐서.)
  [메뉴얼 수행 추적기](/Resource/프로젝트_아이디어/메뉴얼_수행_추적기)
  [유아(6세 이하 초등입학 전)용 동화 읽어주고 각종 질문에 대답해주는 인형](/Resource/프로젝트_아이디어/유아(6세_이하_초등입학_전)용_동화_읽어주고_각종_질문에_대답해주는_인형)
  [정서, 심리 등 과학적 근거를 바탕으로 하는 검사 컨텐츠를 생성형 인공지능으로 서비스하기](/Resource/프로젝트_아이디어/정서,_심리_등_과학적_근거를_바탕으로_하는_검사_컨텐츠를_생성형_인공지능으로_서비스하기)
  ["시끄러운 목소리"가 주목받는 "댓글" 시스템 개선안](/Resource/프로젝트_아이디어/"시끄러운_목소리"가_주목받는_"댓글"_시스템_개선안)


- 구체화된 아이디어
  [anki-diary](/Resource/프로젝트_아이디어/anki-diary)
  [충북프로메이케센터 홈페이지 리뉴얼](/Resource/프로젝트_아이디어/충북프로메이케센터_홈페이지_리뉴얼)


#### TIPS

- 어떤거 만들어볼만한지는 awesome-selfhosted에 적당히 괜찮은 주제들이 있어요

- 게임 아이디어
  [요런 스타일](https://www.youtube.com/watch?v=YPowsippmRM) 좋다.
  [로그라이크류](https://www.youtube.com/watch?v=JFAzdybohUc)도 재밌다.


### Design

- 피그마 스타일 가이드
  [소개](https://www.youtube.com/watch?v=fFNeuHAL3-E)
  [스타일 가이드 소개](https://www.youtube.com/watch?v=_7wO8CZEwBw)


- 학습
  [css 학습 채널](https://www.youtube.com/@lundeveloper)
  [youtube: how to design with figma? (A)](https://www.youtube.com/watch?v=6YpAl-U1ASU)
  [youtube: how to design with figma? (B)](https://www.youtube.com/watch?v=h1gtRXskgoY)
  [youtube: how to design with figma? (C)](https://www.youtube.com/watch?v=l3A9OcUd_Us)


### 유용한 무료 api

- 영영사전
  : 발음, 예문, 동의어, 반의어 등 다양한 정보를 제공한다.
  ['darkness' 사전 검색 요청](https://api.dictionaryapi.dev/api/v2/entries/en/darkness)


### 지능

지능은 다양한 요소로 구성된다.

- 정서
  - 자기 불안을 다스리는 능력

- 사회성
  - 사고의 유연성
  - 추론 능력
  - 문제 해결력


### 학습법

- 무언가를 배우는 방법

  > 1. 공부 하려는 주제에 대해 조사한다.
  > 2. 설명을 적어 본다. 가상의 10살 어린이를 앞에 두고.
  > 3. 잘 모르는 부분, 지나치게 복잡한 부분(전문용어를 사용했다던가)을 찾는다.
  > 4. `3.`에서 찾은 내용을 더 쉽고 단순하게 설명하기 위해 추가적으로 조사한다.


### 대중문화

[보고싶은 영화 리스트](/Resource/대중문화/보고싶은_영화)
[인물](/Resource/대중문화/인물)
[음악](/Resource/대중문화/음악)



### 여행

[가고싶은 식당](/Resource/여행/가고싶은_식당)



### 차량 관리

- 청주시 서원구 기후대기과:
  - 043)201-4985
  - 2024/04/29 17:30 쯤 전화통화

- 청주시 차량등록사업소 세무팀
  - 043)201-4951
  - 2024/04/29 15:37 전화통화

- 올드카 성지: 이화카센터
- 올드카 성지: 서광캬브레타


[비용 기록](/Resource/차량_관리/비용_기록)
[베터리 자가교체](/Resource/차량_관리/베터리_자가교체)
[정기검사지연 과태료와 운행정지명령](/Resource/차량_관리/정기검사지연_과태료와_운행정지명령)
[청주시의 미세먼지 비상저감조치](/Resource/차량_관리/청주시의_미세먼지_비상저감조치)




### etc

[주방기구: 스사모 통즈 구입](/Resource/etc/주방기구:_스사모_통즈_구입)


### REFERENCES

[건전지 사용 제품을 충전지 방식으로 개조하기](https://www.youtube.com/watch?v=-zltotyU1Ek)
[커뮤니티 구축 방법론](https://www.jianyang.co.kr/p/7-feat?utm_source=publication-search)
[better way to manage dot files](https://www.youtube.com/watch?v=tBoLDpTWVOM)
[self hosted secrets manager](https://www.youtube.com/watch?v=7t5M4FXqs9E&list=WL&index=29)
[e-commerce example site](https://contents.clayful.store/)
[good programmer](https://parksb.github.io/article/32.html)

> [강력추천! 영상 크리에이터라면 꼭 알아야 할 7가지 웹사이트]([https://www.youtube.com/watch?v=yYwJI9y_arE)
>
> === Timestamps ===
> 00:00 인트로
> 00:23 레퍼런스 사이트1
> 01:12 레퍼런스 사이트2
> 02:02 레퍼런스 사이트3
> 02:22 레퍼런스 사이트4
> 02:49 레퍼런스 사이트5
> 03:22 레퍼런스 사이트6
> 03:56 레퍼런스 사이트7
> 03:56 아웃트로
>
> ▶레퍼런스 사이트 링크들 입니다.
> https://eyecannndy.com/
> https://kive.ai/discover
> https://film-grab.com/
> https://shot.cafe/
> https://www.cosmos.so/
> nytimes.com/column/anatomy-of-a-scene




## Archive

- Project
  [칭찬 수집기 개발](/Archive/Project/칭찬_수집기)
  [2025 우테코 지원](/Archive/Project/2025_우테코_지원)
  [우정한사랑 이사](/Archive/Project/우정한사랑_이사)
  [gatsby로 블로그](/Archive/Project/gatsby로_블로그/index.md)
  [경주 골든 페스티벌](/Archive/Project/경주_골든_페스티벌)





- commit msg 생성시 --patch 사용 않기
- 3.7 thinkless 모델 사용




#### whichkey

- basic
  - [x] new chat
     - [ ] set name of chat **정말로 필요할까?**

  - [ ] 현재 탭에 모든 버퍼를 컨텍스트로 올리기

- prompt_library
  - 어떤 애들은 `vim.ui.input` 또는 `vim.ui.select`를 받는다.


- ? 버퍼 컨텍스트와 파일 컨텍스트가 다른가?
  `/buffer` vs `/file`




##### inspect chat buf

print(vim.inspect(chat_buf))

- ```
  [ {
      chat = <1>{
        adapter = <2>{
          env = {
            api_key = <function 1>
          },
          features = {
            text = true,
            tokens = true,
            vision = false
          },
          formatted_name = "Copilot",
          handlers = {
            chat_output = <function 2>,
            form_messages = <function 3>,
            form_parameters = <function 4>,
            inline_output = <function 5>,
            on_exit = <function 6>,
            setup = <function 7>,
            tokens = <function 8>
          },
          headers = {
            Authorization = "Bearer ${api_key}",
            ["Content-Type"] = "application/json",
            ["Copilot-Integration-Id"] = "vscode-chat",
            ["Editor-Version"] = "Neovim/0.11.0"
          },
          name = "copilot",
          opts = {
            stream = true
          },
          roles = {
            llm = "assistant",
            user = "user"
          },
          schema = {
            max_tokens = {
              default = 15000,
              desc = "The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
              mapping = "parameters",
              order = 4,
              type = "integer"
            },
            model = {
              choices = <function 9>,
              default = "claude-3.7-sonnet",
              desc = "ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.",
              mapping = "parameters",
              order = 1,
              type = "enum"
            },
            n = {
              condition = <function 10>,
              default = 1,
              desc = "How many chat completions to generate for each prompt.",
              mapping = "parameters",
              order = 6,
              type = "number"
            },
            reasoning_effort = {
              choices = { "high", "medium", "low" },
              default = "medium",
              desc = "Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.",
              mapping = "parameters",
              optional = true,
              order = 2,
              type = "string"
            },
            temperature = {
              condition = <function 11>,
              default = 0,
              desc = "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.",
              mapping = "parameters",
              order = 3,
              type = "number"
            },
            top_p = {
              condition = <function 12>,
              default = 1,
              desc = "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.",
              mapping = "parameters",
              order = 5,
              type = "number"
            }
          },
          url = "https://api.githubcopilot.com/chat/completions",
          <metatable> = {
            __index = <3>{
              extend = <function 13>,
              get_env_vars = <function 14>,
              make_from_schema = <function 15>,
              make_safe = <function 16>,
              map_roles = <function 17>,
              map_schema_to_params = <function 18>,
              new = <function 19>,
              resolve = <function 20>,
              set_env_vars = <function 21>
            }
          }
        },
        agents = {
          aug = 72,
          bufnr = 5,
          chat = {},
          constants = <4>{
            AUTOCMD_GROUP = "codecompanion.agent",
            NS_TOOLS = "CodeCompanion-agents",
            PREFIX = "@",
            PROCESSING_MSG = "Tool processing ...",
            STATUS_ERROR = "error",
            STATUS_SUCCESS = "success"
          },
          extracted = {},
          messages = <5>{ {
              content = "You are an AI programming assistant named \"CodeCompanion\". You are currently plugged into the Neovim text editor on a user's machine.\n\nYour core tasks include:\n- Answering general programming questions.\n- Explaining how the code in a Neovim buffer works.\n- Reviewing the selected code in a Neovim buffer.\n- Generating unit tests for the selected code.\n- Proposing fixes for problems in the selected code.\n- Scaffolding code for a new workspace.\n- Finding relevant code to the user's query.\n- Proposing fixes for test failures.\n- Answering questions about Neovim.\n- Running tools.\n\nYou must:\n- Follow the user's requirements carefully and to the letter.\n- Keep your answers short and impersonal, especially if the user's context is outside your core tasks.\n- Minimize additional prose unless clarification is needed.\n- Use Markdown formatting in your answers.\n- Include the programming language name at the start of each Markdown code block.\n- Avoid including line numbers in code blocks.\n- Avoid wrapping the whole response in triple backticks.\n- Only return code that's directly relevant to the task at hand. You may omit code that isn’t necessary for the solution.\n- Avoid using H1 and H2 headers in your responses.\n- Use actual line breaks in your responses; only use \"\\n\" when you want a literal backslash followed by 'n'.\n- All non-code text responses must be written in the English language indicated.\n\nWhen given a task:\n1. Think step-by-step and, unless the user requests otherwise or the task is very simple, describe your plan in detailed pseudocode.\n2. Output the final code in a single code block, ensuring that only relevant code is included.\n3. End your response with a short suggestion for the next user turn that directly supports continuing the conversation.\n4. Provide exactly one complete reply per conversation turn.",
              cycle = 1,
              id = 1999425959,
              opts = {
                tag = "from_config",
                visible = false
              },
              role = "system"
            } },
          stderr = {},
          stdout = {},
          tool = {},
          tools_config = <6>{
            cmd_runner = {
              callback = "strategies.chat.agents.tools.cmd_runner",
              description = "Run shell commands initiated by the LLM",
              opts = {
                requires_approval = true
              }
            },
            editor = {
              callback = "strategies.chat.agents.tools.editor",
              description = "Update a buffer with the LLM's response"
            },
            files = {
              callback = "strategies.chat.agents.tools.files",
              description = "Update the file system with the LLM's response",
              opts = {
                requires_approval = true
              }
            },
            groups = {
              full_stack_dev = {
                description = "Full Stack Developer - Can run code, edit code and modify files",
                system_prompt = "**DO NOT** make any assumptions about the dependencies that a user has installed. If you need to install any dependencies to fulfil the user's request, do so via the Command Runner tool. If the user doesn't specify a path, use their current working directory.",
                tools = { "cmd_runner", "editor", "files" }
              }
            },
            opts = {
              auto_submit_errors = false,
              auto_submit_success = false,
              system_prompt = "## Tools Access and Execution Guidelines\n\n### Overview\nYou now have access to specialized tools that empower you to assist users with specific tasks. These tools are available only when explicitly requested by the user.\n\n### General Rules\n- **User-Triggered:** Only use a tool when the user explicitly indicates that a specific tool should be employed (e.g., phrases like \"run command\" for the cmd_runner).\n- **Strict Schema Compliance:** Follow the exact XML schema provided when invoking any tool.\n- **XML Format:** Always wrap your responses in a markdown code block designated as XML and within the `<tools></tools>` tags.\n- **Valid XML Required:** Ensure that the constructed XML is valid and well-formed.\n- **Multiple Commands:**\n  - If issuing commands of the same type, combine them within one `<tools></tools>` XML block with separate `<action></action>` entries.\n  - If issuing commands for different tools, ensure they're wrapped in `<tool></tool>` tags within the `<tools></tools>` block.\n- **No Side Effects:** Tool invocations should not alter your core tasks or the general conversation structure."
            }
          },
          tools_ns = 39,
          <metatable> = {
            __index = <7>{
              add_error_to_chat = <function 22>,
              execute = <function 23>,
              find = <function 24>,
              fold_xml = <function 25>,
              new = <function 26>,
              parse = <function 27>,
              parse_buffer = <function 28>,
              replace = <function 29>,
              reset = <function 30>,
              resolve = <function 31>,
              set_autocmds = <function 32>
            }
          }
        },
        aug = 71,
        bufnr = 5,
        context = <8>{
          bufnr = 3,
          buftype = "nofile",
          cursor_pos = { 3, 2 },
          end_col = 2,
          end_line = 3,
          filename = "/home/sy/[CodeCompanion] 3528433",
          filetype = "codecompanion",
          is_normal = true,
          is_visual = false,
          line_count = 3,
          lines = {},
          mode = "n",
          start_col = 2,
          start_line = 3,
          winnr = 1003
        },
        create_buf = <function 33>,
        cycle = 1,
        from_prompt_library = false,
        header_line = 1,
        id = 5436880,
        last_role = "user",
        messages = <table 5>,
        opts = {
          auto_submit = false,
          context = <table 8>
        },
        parser = <9>{
          _callbacks = {
            bytes = {},
            changedtree = {},
            child_added = {},
            child_removed = {},
            detach = { <function 34> }
          },
          _callbacks_rec = {
            bytes = {},
            changedtree = { <function 35> },
            child_added = { <function 36> },
            child_removed = { <function 37> },
            detach = {}
          },
          _cb_queues = {},
          _children = {
            markdown_inline = {
              _callbacks = {
                bytes = {},
                changedtree = {},
                child_added = {},
                child_removed = {},
                detach = {}
              },
              _callbacks_rec = {
                bytes = {},
                changedtree = { <function 35> },
                child_added = { <function 36> },
                child_removed = { <function 37> },
                detach = {}
              },
              _cb_queues = {},
              _children = {},
              _injection_query = <10>{
                _processed_patterns = { {
                    directives = { <11>{ "set!", "injection.language", "html" }, <12>{ "set!", "injection.combined" } },
                    predicates = {}
                  }, {
                    directives = { <13>{ "set!", "injection.language", "latex" }, <14>{ "set!", "injection.include-children" } },
                    predicates = {}
                  } },
                captures = <15>{ "injection.content" },
                has_combined_injections = true,
                info = {
                  captures = <table 15>,
                  patterns = { { <table 11>, <table 12> }, { <table 13>, <table 14> } }
                },
                lang = "markdown_inline",
                query = <userdata 1>,
                <metatable> = <16>{
                  __index = <table 16>,
                  _apply_directives = <function 38>,
                  _match_predicates = <function 39>,
                  _process_patterns = <function 40>,
                  iter_captures = <function 41>,
                  iter_matches = <function 42>,
                  new = <function 43>
                }
              },
              _is_entirely_valid = true,
              _lang = "markdown_inline",
              _num_regions = 2,
              _num_valid_regions = 2,
              _opts = <17>{
                error = false
              },
              _parent = <table 9>,
              _parser = <userdata 2>,
              _processed_injection_range = <18>{ 0, inf },
              _ranges_being_parsed = {},
              _regions = { { { 0, 3, 3, 0, 11, 11 } }, { { 2, 0, 13, 2, 2, 15 } } },
              _source = 5,
              _trees = { <userdata 3>, <userdata 4> },
              _valid_regions = { true, true },
              <metatable> = <19>{
                __index = <table 19>,
                _add_injections = <function 44>,
                _async_parse = <function 45>,
                _do_callback = <function 46>,
                _edit = <function 47>,
                _get_injection = <function 48>,
                _get_injections = <function 49>,
                _iter_regions = <function 50>,
                _log = <function 51>,
                _on_bytes = <function 52>,
                _on_detach = <function 53>,
                _on_reload = <function 54>,
                _parse = <function 55>,
                _parse_regions = <function 56>,
                _push_async_callback = <function 57>,
                _run_async_callbacks = <function 58>,
                _set_logger = <function 59>,
                _subtract_time = <function 60>,
                add_child = <function 61>,
                children = <function 62>,
                contains = <function 63>,
                destroy = <function 64>,
                for_each_tree = <function 65>,
                included_regions = <function 66>,
                invalidate = <function 67>,
                is_valid = <function 68>,
                lang = <function 69>,
                language_for_range = <function 70>,
                named_node_for_range = <function 71>,
                new = <function 72>,
                node_for_range = <function 73>,
                parent = <function 74>,
                parse = <function 75>,
                register_cbs = <function 76>,
                remove_child = <function 77>,
                set_included_regions = <function 78>,
                source = <function 79>,
                tree_for_range = <function 80>,
                trees = <function 81>
              }
            }
          },
          _injection_query = <20>{
            _processed_patterns = { {
                directives = { <21>{ "set-lang-from-info-string!", 1 } },
                predicates = {}
              }, {
                directives = { <22>{ "set!", "injection.language", "html" }, <23>{ "set!", "injection.combined" }, <24>{ "set!", "injection.include-children" } },
                predicates = {}
              }, {
                directives = { <25>{ "set!", "injection.language", "yaml" }, <26>{ "offset!", 2, "1", "0", "-1", "0" }, <27>{ "set!", "injection.include-children" } },
                predicates = {}
              }, {
                directives = { <28>{ "set!", "injection.language", "toml" }, <29>{ "offset!", 2, "1", "0", "-1", "0" }, <30>{ "set!", "injection.include-children" } },
                predicates = {}
              }, {
                directives = { <31>{ "set!", "injection.language", "markdown_inline" } },
                predicates = {}
              } },
            captures = <32>{ "_lang", "injection.content" },
            has_combined_injections = true,
            info = {
              captures = <table 32>,
              patterns = { { <table 21> }, { <table 22>, <table 23>, <table 24> }, { <table 25>, <table 26>, <table 27> }, { <table 28>, <table 29>, <table 30> }, { <table 31> } }
            },
            lang = "markdown",
            query = <userdata 5>,
            <metatable> = <table 16>
          },
          _is_entirely_valid = true,
          _lang = "markdown",
          _num_regions = 1,
          _num_valid_regions = 1,
          _opts = <table 17>,
          _parser = <userdata 6>,
          _processed_injection_range = <table 18>,
          _ranges_being_parsed = {},
          _source = 5,
          _trees = { <userdata 7> },
          _valid_regions = { true },
          <metatable> = <table 19>
        },
        position = {
          col = 1,
          line = "",
          row = 3
        },
        references = {
          Chat = <table 1>,
          <metatable> = {
            __index = <33>{
              add = <function 82>,
              can_be_pinned = <function 83>,
              can_be_watched = <function 84>,
              clear = <function 85>,
              get_from_chat = <function 86>,
              make_id_from_buf = <function 87>,
              new = <function 88>,
              render = <function 89>
            }
          }
        },
        refs = {},
        settings = <34>{
          max_tokens = 15000,
          model = "claude-3.7-sonnet",
          n = 1,
          reasoning_effort = "medium",
          temperature = 0,
          top_p = 1
        },
        status = "",
        subscribers = {
          queue = {},
          stopped = false,
          <metatable> = {
            __index = <35>{
              action = <function 90>,
              has_subscribers = <function 91>,
              new = <function 92>,
              process = <function 93>,
              stop = <function 94>,
              submit = <function 95>,
              subscribe = <function 96>,
              unsubscribe = <function 97>
            }
          }
        },
        tool_flags = {},
        tools_in_use = {},
        ui = {
          adapter = <table 2>,
          aug = 73,
          bufnr = 5,
          header_ns = 40,
          id = 5436880,
          intro_message = true,
          roles = {
            llm = <function 98>,
            user = " 󰟷"
          },
          settings = <table 34>,
          virtual_text_ns = 42,
          winnr = 1009,
          <metatable> = {
            __index = <36>{
              clear_virtual_text = <function 99>,
              display_tokens = <function 100>,
              fold_code = <function 101>,
              follow = <function 102>,
              format_header = <function 103>,
              hide = <function 104>,
              is_active = <function 105>,
              is_visible = <function 106>,
              last = <function 107>,
              lock_buf = <function 108>,
              new = <function 109>,
              open = <function 110>,
              render = <function 111>,
              render_headers = <function 112>,
              set_header = <function 113>,
              set_intro_msg = <function 114>,
              set_virtual_text = <function 115>,
              unlock_buf = <function 116>
            }
          }
        },
        variables = {
          vars = <37>{
            buffer = {
              callback = "strategies.chat.variables.buffer",
              description = "Share the current buffer with the LLM",
              opts = {
                contains_code = true,
                has_params = true
              }
            },
            lsp = {
              callback = "strategies.chat.variables.lsp",
              description = "Share LSP information and code for the current buffer",
              opts = {
                contains_code = true
              }
            },
            viewport = {
              callback = "strategies.chat.variables.viewport",
              description = "Share the code that you see in Neovim with the LLM",
              opts = {
                contains_code = true
              }
            }
          },
          <metatable> = {
            __index = <38>{
              find = <function 117>,
              new = <function 118>,
              parse = <function 119>,
              replace = <function 120>
            }
          }
        },
        watchers = {
          augroup = 67,
          buffers = {},
          <metatable> = {
            __index = <39>{
              check_for_changes = <function 121>,
              get_changes = <function 122>,
              new = <function 123>,
              unwatch = <function 124>,
              watch = <function 125>
            }
          }
        },
        <metatable> = {
          __index = <40>{
            _get_settings_key = <function 126>,
            add_buf_message = <function 127>,
            add_message = <function 128>,
            add_pins = <function 129>,
            add_reference = <function 130>,
            add_system_prompt = <function 131>,
            add_tool = <function 132>,
            apply_model = <function 133>,
            apply_settings = <function 134>,
            apply_tools_and_variables = <function 135>,
            buf_get_chat = <function 136>,
            check_references = <function 137>,
            clear = <function 138>,
            close = <function 139>,
            close_last_chat = <function 140>,
            complete_models = <function 141>,
            debug = <function 142>,
            done = <function 143>,
            get_codeblock = <function 144>,
            get_messages = <function 145>,
            has_tools = <function 146>,
            has_user_messages = <function 147>,
            increment_cycle = <function 148>,
            last_chat = <function 149>,
            new = <function 150>,
            parse_msg_for_vars = <function 151>,
            regenerate = <function 152>,
            remove_tagged_message = <function 153>,
            reset = <function 154>,
            set_autocmds = <function 155>,
            set_range = <function 156>,
            stop = <function 157>,
            submit = <function 158>,
            toggle_system_prompt = <function 159>
          }
        }
      },
      description = "[No messages]",
      name = "Chat 2",
      strategy = "chat"
    },
  {
      chat = <41>{
        adapter = <42>{
          env = {
            api_key = <function 1>
          },
          features = {
            text = true,
            tokens = true,
            vision = false
          },
          formatted_name = "Copilot",
          handlers = {
            chat_output = <function 2>,
            form_messages = <function 3>,
            form_parameters = <function 4>,
            inline_output = <function 5>,
            on_exit = <function 6>,
            setup = <function 7>,
            tokens = <function 8>
          },
          headers = {
            Authorization = "Bearer ${api_key}",
            ["Content-Type"] = "application/json",
            ["Copilot-Integration-Id"] = "vscode-chat",
            ["Editor-Version"] = "Neovim/0.11.0"
          },
          name = "copilot",
          opts = {
            stream = true
          },
          parameters = {
            max_tokens = 15000,
            model = "claude-3.7-sonnet",
            n = 1,
            reasoning_effort = "medium",
            temperature = 0,
            top_p = 1
          },
          roles = {
            llm = "assistant",
            user = "user"
          },
          schema = {
            max_tokens = {
              default = 15000,
              desc = "The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.",
              mapping = "parameters",
              order = 4,
              type = "integer"
            },
            model = {
              choices = <function 9>,
              default = "claude-3.7-sonnet",
              desc = "ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.",
              mapping = "parameters",
              order = 1,
              type = "enum"
            },
            n = {
              condition = <function 10>,
              default = 1,
              desc = "How many chat completions to generate for each prompt.",
              mapping = "parameters",
              order = 6,
              type = "number"
            },
            reasoning_effort = {
              choices = { "high", "medium", "low" },
              default = "medium",
              desc = "Constrains effort on reasoning for reasoning models. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.",
              mapping = "parameters",
              optional = true,
              order = 2,
              type = "string"
            },
            temperature = {
              condition = <function 11>,
              default = 0,
              desc = "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.",
              mapping = "parameters",
              order = 3,
              type = "number"
            },
            top_p = {
              condition = <function 12>,
              default = 1,
              desc = "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.",
              mapping = "parameters",
              order = 5,
              type = "number"
            }
          },
          url = "https://api.githubcopilot.com/chat/completions",
          <metatable> = {
            __index = <table 3>
          }
        },
        agents = {
          aug = 66,
          bufnr = 3,
          chat = {},
          constants = <table 4>,
          extracted = {},
          messages = <43>{ {
              content = "You are an AI programming assistant named \"CodeCompanion\". You are currently plugged into the Neovim text editor on a user's machine.\n\nYour core tasks include:\n- Answering general programming questions.\n- Explaining how the code in a Neovim buffer works.\n- Reviewing the selected code in a Neovim buffer.\n- Generating unit tests for the selected code.\n- Proposing fixes for problems in the selected code.\n- Scaffolding code for a new workspace.\n- Finding relevant code to the user's query.\n- Proposing fixes for test failures.\n- Answering questions about Neovim.\n- Running tools.\n\nYou must:\n- Follow the user's requirements carefully and to the letter.\n- Keep your answers short and impersonal, especially if the user's context is outside your core tasks.\n- Minimize additional prose unless clarification is needed.\n- Use Markdown formatting in your answers.\n- Include the programming language name at the start of each Markdown code block.\n- Avoid including line numbers in code blocks.\n- Avoid wrapping the whole response in triple backticks.\n- Only return code that's directly relevant to the task at hand. You may omit code that isn’t necessary for the solution.\n- Avoid using H1 and H2 headers in your responses.\n- Use actual line breaks in your responses; only use \"\\n\" when you want a literal backslash followed by 'n'.\n- All non-code text responses must be written in the English language indicated.\n\nWhen given a task:\n1. Think step-by-step and, unless the user requests otherwise or the task is very simple, describe your plan in detailed pseudocode.\n2. Output the final code in a single code block, ensuring that only relevant code is included.\n3. End your response with a short suggestion for the next user turn that directly supports continuing the conversation.\n4. Provide exactly one complete reply per conversation turn.",
              cycle = 1,
              id = 1999425959,
              opts = {
                tag = "from_config",
                visible = false
              },
              role = "system"
            }, {
              content = "hi:",
              cycle = 1,
              id = 368182401,
              opts = {
                visible = true
              },
              role = "user"
            }, {
              content = "Hello! I'm CodeCompanion, your AI programming assistant in Neovim. I can help you with:\n\n- Answering programming questions\n- Explaining code\n- Code reviews\n- Writing unit tests\n- Fixing code issues\n- Creating new projects\n- Finding relevant code\n- Fixing test failures\n- Neovim assistance\n\nHow can I assist with your programming tasks today?",
              cycle = 1,
              id = 670311906,
              opts = {
                visible = true
              },
              role = "llm"
            } },
          stderr = {},
          stdout = {},
          tool = {},
          tools_config = <table 6>,
          tools_ns = 39,
          <metatable> = {
            __index = <table 7>
          }
        },
        aug = 65,
        bufnr = 3,
        context = <44>{
          bufnr = 1,
          buftype = "nofile",
          cursor_pos = { 7, 98 },
          end_col = 98,
          end_line = 7,
          filename = "",
          filetype = "alpha",
          is_normal = true,
          is_visual = false,
          line_count = 48,
          lines = {},
          mode = "n",
          start_col = 98,
          start_line = 7,
          winnr = 1000
        },
        create_buf = <function 160>,
        cycle = 2,
        from_prompt_library = false,
        header_line = 21,
        id = 3528433,
        last_role = "user",
        messages = <table 43>,
        opts = {
          auto_submit = false,
          context = <table 44>
        },
        parser = <45>{
          _callbacks = {
            bytes = {},
            changedtree = {},
            child_added = {},
            child_removed = {},
            detach = { <function 161>, <function 162> }
          },
          _callbacks_rec = {
            bytes = {},
            changedtree = { <function 163>, <function 164> },
            child_added = { <function 165>, <function 166> },
            child_removed = { <function 167>, <function 168> },
            detach = {}
          },
          _cb_queues = {},
          _children = {
            markdown_inline = {
              _callbacks = {
                bytes = {},
                changedtree = {},
                child_added = {},
                child_removed = {},
                detach = {}
              },
              _callbacks_rec = {
                bytes = {},
                changedtree = { <function 163>, <function 164> },
                child_added = { <function 165>, <function 166> },
                child_removed = { <function 167>, <function 168> },
                detach = {}
              },
              _cb_queues = {},
              _children = {},
              _injection_query = <table 10>,
              _is_entirely_valid = true,
              _lang = "markdown_inline",
              _num_regions = 15,
              _num_valid_regions = 15,
              _opts = <46>{
                error = false
              },
              _parent = <table 45>,
              _parser = <userdata 8>,
              _processed_injection_range = <table 18>,
              _ranges_being_parsed = {},
              _regions = { { { 0, 3, 3, 0, 11, 11 } }, { { 2, 0, 13, 2, 3, 16 } }, { { 4, 4, 22, 4, 22, 40 } }, { { 6, 0, 42, 6, 87, 129 } }, { { 8, 2, 133, 8, 33, 164 } }, { { 9, 2, 167, 9, 17, 182 } }, { { 10, 2, 185, 10, 14, 197 } }, { { 11, 2, 200, 11, 20, 218 } }, { { 12, 2, 221, 12, 20, 239 } }, { { 13, 2, 242, 13, 23, 263 } }, { { 14, 2, 266, 14, 23, 287 } }, { { 15, 2, 290, 15, 22, 310 } }, { { 16, 2, 313, 16, 19, 330 } }, { { 18, 0, 332, 18, 51, 383 } }, { { 20, 3, 388, 20, 11, 396 } } },
              _source = 3,
              _trees = { <userdata 9>, <userdata 10>, <userdata 11>, <userdata 12>, <userdata 13>, <userdata 14>, <userdata 15>, <userdata 16>, <userdata 17>, <userdata 18>, <userdata 19>, <userdata 20>, <userdata 21>, <userdata 22>, <userdata 23> },
              _valid_regions = { true, true, true, true, true, true, true, true, true, true, true, true, true, true, true },
              <metatable> = <table 19>
            }
          },
          _injection_query = <table 20>,
          _is_entirely_valid = true,
          _lang = "markdown",
          _num_regions = 1,
          _num_valid_regions = 1,
          _opts = <table 46>,
          _parser = <userdata 24>,
          _processed_injection_range = <table 18>,
          _ranges_being_parsed = {},
          _source = 3,
          _trees = { <userdata 25> },
          _valid_regions = { true },
          <metatable> = <table 19>
        },
        position = {
          col = 1,
          line = "",
          row = 23
        },
        references = {
          Chat = <table 41>,
          <metatable> = {
            __index = <table 33>
          }
        },
        refs = {},
        settings = <47>{
          max_tokens = 15000,
          model = "claude-3.7-sonnet",
          n = 1,
          reasoning_effort = "medium",
          temperature = 0,
          top_p = 1
        },
        status = "",
        subscribers = {
          queue = {},
          stopped = false,
          <metatable> = {
            __index = <table 35>
          }
        },
        tool_flags = {},
        tools_in_use = {},
        ui = {
          adapter = <table 42>,
          aug = 68,
          bufnr = 3,
          header_ns = 40,
          id = 3528433,
          intro_message = true,
          roles = {
            llm = <function 98>,
            user = " 󰟷"
          },
          settings = <table 47>,
          tokens = 471,
          virtual_text_ns = 42,
          winnr = 1008,
          <metatable> = {
            __index = <table 36>
          }
        },
        variables = {
          vars = <table 37>,
          <metatable> = {
            __index = <table 38>
          }
        },
        watchers = {
          augroup = 67,
          buffers = {},
          <metatable> = {
            __index = <table 39>
          }
        },
        <metatable> = {
          __index = <table 40>
        }
      },
      description = "[No messages]",
      name = "Chat 1",
      strategy = "chat"
    } ]
  ```

#### slash_command vs variable vs tools vs prompt_library vs workflow

- tools: **comming soon...**

- variable: static 요소들 alias 지정

- slash_command: context 정의용으로만 사용하자. 적어도 variable보다는 쓸모가 있다.
  - /workspace 활용하는 방법만 좀 익혀두고, 토큰량 고려해 항상 pin하거나 watch하면 괜찮을듯
    - vectorcode plugin 설치
  - file, buffer, fetch

- prompt_library: 미리 정의된 프롬프트, file, url 참조 가능
  - which-key 단축키로 지정해서 사용하기 용이함 `ex> Codecompanion.prompt("my-prompt")`
  - 필요시 slash_command에 등록할 수도 있음(**별도로 slash command를 만들 필요가 없다!**)
  - inline에서 처리될수도 있고, chat으로 갈수도 있다.
  - 기본제공되는것을 없앨수는 없는 모양이다. 어차피 프롬프트는 keymap으로 만 활용하니 상관 없다.


- AND WHAT? workflow ??




#### mcp

**WTF is mcp, why ppl run for this?**
